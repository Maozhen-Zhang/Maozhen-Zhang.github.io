<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>深度学习代码理解 | Maozhen Zhang</title> <meta name="author" content="Maozhen Zhang"> <meta name="description" content="comprehend python and deep learning come"> <meta name="keywords" content="medical-imaging, machine-learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%EF%A3%BF&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://maozhenzhang.github.io/blog/2023/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E7%90%86%E8%A7%A3/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Maozhen </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">深度学习代码理解</h1> <p class="post-meta">February 28, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/code"> <i class="fas fa-hashtag fa-sm"></i> Code</a>     ·   <a href="/blog/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> Deep-Learning</a>   </p> </header> <article class="post-content"> <h1 id="深度学习">深度学习</h1> <h2 id="卷神经网络pytorch">卷神经网络pytorch</h2> <p><strong>数据</strong></p> <ol> <li> <p>数据处理(os，torch.utils.data.Dataset)</p> </li> <li> <p>数据变换(torchvision.transforms)</p> </li> <li> <p>数据增强(mixup拼接,cutout裁剪,cutmix融合前面两种方法)</p> </li> </ol> <p><strong>模型</strong></p> <ol> <li>Torchvision.model</li> </ol> <p><strong>优化器及loss</strong></p> <ol> <li>optimizer</li> <li>loss</li> </ol> <p><strong>训练</strong></p> <h2 id="数据集构建">数据集构建</h2> <p>使用自己的<code class="language-plaintext highlighter-rouge">x_train</code>和<code class="language-plaintext highlighter-rouge">y_train</code>构建数据集</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#继承并重写
</span><span class="k">class</span> <span class="nc">Mydataset</span><span class="p">(</span><span class="n">Data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span><span class="c1">#将变量放入
</span>    <span class="n">self</span><span class="p">.</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span>
    <span class="n">self</span><span class="p">.</span><span class="n">y_label</span> <span class="o">=</span> <span class="n">y_label</span>
  <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">x_train</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">y_label</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span><span class="c1">#返回长度
</span>    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">)</span>
 
<span class="c1">#使用 MyDataset，一次只能获取一个样本
#使用了Dataloader，一次能获取一批
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">Data</span><span class="p">.</span><span class="nc">Dataloader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="dataloader详解">DataLoader详解</h3> <h3 id="使用">使用：</h3> <ol> <li>创建一个<code class="language-plaintext highlighter-rouge">Dataset</code>对象</li> <li>创建一个<code class="language-plaintext highlighter-rouge">DataLoader</code>对象</li> <li>循环这个<code class="language-plaintext highlighter-rouge">DataLoader</code>对象，将xx、xx加载到模型中训练</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="n">batch_sampler</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="n">pin_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</code></pre></div></div> <blockquote> <p><code class="language-plaintext highlighter-rouge">dataset(Dataset)</code>: 传入的数据集 <code class="language-plaintext highlighter-rouge">batch_size(int, optional)</code>: 每个batch有多少个样本 <code class="language-plaintext highlighter-rouge">shuffle(bool, optional)</code>: 在每个epoch开始的时候，对数据进行重新排序 <code class="language-plaintext highlighter-rouge">sampler(Sampler, optional)</code>: 自定义从数据集中取样本的策略，如果指定这个参数，那么shuffle必须为False <code class="language-plaintext highlighter-rouge">batch_sampler(Sampler, optional)</code>: 与sampler类似，但是一次只返回一个batch的indices（索引），需要注意的是，一旦指定了这个参数，那么batch_size,shuffle,sampler,drop_last就不能再指定了（互斥——Mutually exclusive） <code class="language-plaintext highlighter-rouge">num_workers (int, optional)</code>: 这个参数决定了有几个进程来处理data loading。0意味着所有的数据都会被load进主进程。（默认为0） <code class="language-plaintext highlighter-rouge">collate_fn (callable, optional)</code>: 将一个list的sample组成一个mini-batch的函数 <code class="language-plaintext highlighter-rouge">pin_memory (bool, optional)</code>： 如果设置为True，那么data loader将会在返回它们之前，将tensors拷贝到CUDA中的固定内存（CUDA pinned memory）中. <code class="language-plaintext highlighter-rouge">drop_last (bool, optional)</code>: 如果设置为True：这个是对最后的未完成的batch来说的，比如你的batch_size设置为64，而一个epoch只有100个样本，那么训练的时候后面的36个就被扔掉了… 如果为False（默认），那么会继续正常执行，只是最后的batch_size会小一点。 <code class="language-plaintext highlighter-rouge">timeout(numeric, optional)</code>: 如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。默认为0</p> </blockquote> <h2 id="模型构建">模型构建</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Lineear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span><span class="c1">#可以改写非耦合
</span>    <span class="n">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">lieanr</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>	<span class="c1">#=&gt;[batch_size, 64]
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>	<span class="c1">#=&gt;[batch_size,64]
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>	<span class="c1">#=&gt;[batch_size,8]
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lienar3</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>	<span class="c1">#=&gt;[batch_size,4]
</span>   
<span class="n">model</span> <span class="o">=</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyloss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">()</span>

</code></pre></div></div> <h2 id="模型运行">模型运行</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Epoch</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">Epoch</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1">#这里期望要long类型，y的类型需要注意
</span>    
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
</code></pre></div></div> <p>模型训练时整体内容：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#外层epoch循环
</span><span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">epochs</span><span class="p">):</span>
  <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">#记录每轮epoch的损失
</span>  <span class="c1">#遍历dataloader，里面的data是batch
</span>  <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="c1">#data[0]是输入数据，data[1]是标签
</span>    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  
    <span class="c1">#一些方法的获取和初始化
</span>    <span class="c1">#损失函数
</span>    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># 交叉熵损失
</span>    <span class="c1">#优化器&gt;&gt;&gt;实现随机梯度下降算法,lr– 学习率,momentum– 动量因子
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>  
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span><span class="c1">#调用backward()之前要将梯度清零
</span>  
		<span class="c1">#向前、向后、优化
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>							<span class="c1">#前向
</span>		<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>	<span class="c1">#求损失操作：将输出和标签输入
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>										<span class="c1">#损失向输入测进行反向传播&gt;&gt;&gt;将梯度积累到x.grad中备用
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>									<span class="c1">#优化器对x值进行更新，
</span></code></pre></div></div> <h1 id="自动编码器">自动编码器</h1> <p>自动编码器是神经网络中的一种，基本思想是使用一层或多层神经网络对输入数据进行行摄，得到输出向量，作为从输入数据提取出的特征。</p> <p>传统自编码器一般用来数据降维或着特征学习，如PCA，但是自动编码器比PCA灵活，它既能表征线性变换又能表征非线性变换。</p> <p>自动编码器可以看做是前馈网络的一个特例。基本的子编码器是一个简单的三层神经网络结构：一个输入层、一个隐藏层和一个输出层，其中输出层和输入层具有相同的维数。</p> <p>自编码器，它的输入输出是一致的，目标是使用稀琉的高阶特征重新组合来重构自己。自动编码器是一种数据压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习。</p> <p>目前自编码器的两个主要用途就是降维、去噪和图像生成。</p> <h2 id="类别">类别</h2> <p>欠完备自编码器</p> <blockquote> <p>vanilla自编码器、多层自编码器、卷积子编码器CAE</p> </blockquote> <p>正则编码器</p> <blockquote> <p>稀疏子编码器、去噪子编码器DAE、收缩自编码器</p> </blockquote> <p>变分自编码器VAE</p> <h2 id="欠完备自编码器">欠完备自编码器</h2> <p>自编码器将数据作为输入并发现数据的一些潜在状态表示的模型（欠完备，洗漱，降噪，收缩等），将输入的数据转换为一个编码向量，其中每个维度表示一些学到的关于数据的属性。最重要的细节就是编码器网络为每个编码器维度输出单个值，而解码器网络随后接收这些值并尝试重构原始输入。</p> <h2 id="变分自编码器">变分自编码器</h2> <p>变分子编码器(Variational Auto-Encoder,VAE)以概率的方式描述潜在空间观察。将给定输入的每个潜在属性表示为分布概率。</p> <p>举个例子，中间的单值即为传统自编码器的方式，而变分自编码器使用概率属于来描述潜在属性，类似于GAN</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230309201215096.png" alt="image-20230309201215096" style="zoom:50%;"></p> <p>通过构造编码器模型来输出可能的范围(统计分布)，然后随机采样这些值供给解码器模型，实现了连续、平滑的潜在空间表示，对于潜在分布的所有采样，期望解码器模型能够准确准确重构输入， 因此，在潜在空间中彼此相邻的值应该有非常类似的重构对应。</p> <p>希望通过构建隐变量\(z\)生成目标数据\(x\)，但是只能看到\(x\)，想要推断出\(z\)的特征，这就是贝叶斯概率： \(p(z|x)=\frac{p(x|z)p(z)}{p(x)}\) 然而计算\(p(x)\)是一个非常复杂的过程： \(p(x)=\int p(x|z)p(z)dz\) 它通常是一个复杂的分布，可以饮用变分推断来估计这个值，因此，VAE的理论基础就是变分与贝叶斯。不过可以使用另外一个分布\(q(z|x)\)来金丝\(p(z|x)\)，将其定义为具有可伸缩的分布，使用\(KL\)散度来度量两个概率分布的差值， \(\min KL(q(z|x)||p(x|z))\) 推导结果，可以通过最大化下面的式子来最小化上述表达式： \(E_{q(x|z)}\log p(x|z)-KL(q(z|x)||p(z))\) 第一个式子代表重构的可能性，第二个确保学习的分布\(q\)类似于真实的先验分布。</p> <h3 id="模型细节">模型细节</h3> <ul> <li> <p>VAE编码器模型将输出描述在空间中每个维度分布的参数，假设先验符合正态分布，输出两个向量描述潜在状态的均值和方差。</p> </li> <li>构建一个真正的多元高斯模型，需要定义一个协方差矩阵来描述每个维度是如何相关的。做一个简化的假设，使协方差矩阵对角线上只有非零值，允许用简单的向量描述这些信息。</li> <li> </li> </ul> <h1 id="代码理解">代码理解</h1> <h2 id="细节">细节</h2> <h5 id="lossbackward理解">loss.backward()理解</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="c1">#即
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div> <p>将损失loss向输入侧进行反向传播，同时对于需要进行梯度计算的所有变量\(x(requires_grad=True)\)，计算梯度\(\frac{d}{dx}loss\)并将其累积到梯度\(x.grad\)中备用，即</p> \[x.grad = x.grad+\frac{d}{dx}loss\] <h5 id="optimizerstep理解">optimizer.step()理解</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>  
<span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>	<span class="c1">#调用backward()之前要将梯度清零,因为使其在每个batch中不进行累计
</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>是优化器对x的值进行更新，以随机梯度下降SGD为例子：</p> <blockquote> <p>学习率(learning rate, lr)来控制步幅，即\(x = x - lr * x.grad\)，减号是因为沿着梯度反方向调整变量以减少cost</p> </blockquote> <h5 id="optimizerzero_grad理解">optimizer.zero_grad()理解</h5> <blockquote> <p>Pytorch 为什么每一轮batch需要设置？</p> </blockquote> <p>根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad 了。</p> <p>还可以补充的一点是，如果不是每一个batch就清除掉原有的梯度，而是比如说两个batch再清除掉梯度，这是一种变相提高batch_size的方法，对于计算机硬件不行，但是batch_size可能需要设高的领域比较适合，比如目标检测模型的训练</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Maozhen Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 12, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>