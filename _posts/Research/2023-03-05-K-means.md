---
layout: post
title: K-Means
date: 2023-02-28 12:52:00
description: k-means代码及理解
tags: Research
categories: Machine-Learning
---



# K-means

[知乎](https://github.com/apachecn/ailearning/blob/master/docs/ml/10.md)

## 代码

### K-means

```python

# 计算两个向量的欧式距离（可根据场景选择其他距离公式）
def distEclud(vecA, vecB):
    return sqrt(sum(power(vecA - vecB, 2)))  # la.norm(vecA-vecB)


# 为给定数据集构建一个包含 k 个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成。然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。
def randCent(dataSet, k):
    n = shape(dataSet)[1]  # 列的数量，即数据的特征个数
    centroids = mat(zeros((k, n)))  # 创建k个质心矩阵
    for j in range(n):  # 创建随机簇质心，并且在每一维的边界内
        minJ = min(dataSet[:, j])  # 最小值
        rangeJ = float(max(dataSet[:, j]) - minJ)  # 范围 = 最大值 - 最小值
        centroids[:, j] = mat(minJ + rangeJ * random.rand(k, 1))  # 随机生成，mat为numpy函数，需要在最开始写上 from numpy import *
    return centroids


# k-means 聚类算法
# 该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。
# 这个过程重复数次，直到数据点的簇分配结果不再改变位置。
# 运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）
def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    m = shape(dataSet)[0]  # 行数，即数据个数
    clusterAssment = mat(zeros((m, 2)))  # 创建一个与 dataSet 行数一样，但是有两列的矩阵，用来保存簇分配结果
    centroids = createCent(dataSet, k)  # 创建质心，随机k个质心
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(m):  # 循环每一个数据点并分配到最近的质心中去
            minDist = inf;
            minIndex = -1
            for j in range(k):
                distJI = distMeas(centroids[j, :], dataSet[i, :])  # 计算数据点到质心的距离
                if distJI < minDist:  # 如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）
                    minDist = distJI;
                    minIndex = j
            if clusterAssment[i, 0] != minIndex:  # 簇分配结果改变
                clusterChanged = True  # 簇改变
                clusterAssment[i, :] = minIndex, minDist ** 2  # 更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方
        for cent in range(k):  # 更新质心
            ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A == cent)[0]]  # 获取该簇中的所有点
            centroids[cent, :] = mean(ptsInClust, axis=0)  # 将质心修改为簇中所有点的平均值，mean 就是求平均值的
    return centroids, clusterAssment
```

### DBI

```python
import math
 
# nc is number of clusters
# to be implemented without the use of any libraries (from the scratch)
 
def vectorDistance(v1, v2):
    """
    this function calculates de euclidean distance between two
    vectors.
    """
    sum = 0
    for i in range(len(v1)):
        sum += (v1[i] - v2[i]) ** 2
    return sum ** 0.5
 
def compute_Si(i, x, clusters,nc):
    norm_c = nc
    s = 0
    for t in x[i]:
        s += vectorDistance(t,clusters)
    return s/norm_c
 
def compute_Rij(i, j, x, clusters, nc):
    Mij = vectorDistance(clusters[i],clusters[j])
    Rij = (compute_Si(i,x,clusters[i],nc) + compute_Si(j,x,clusters[j],nc))/Mij
    return Rij
 
def compute_Di(i, x, clusters, nc):
    list_r = []
    for j in range(nc):
        if i != j:
            temp = compute_Rij(i, j, x, clusters, nc)
            list_r.append(temp)
    return max(list_r)
 
def compute_DB_index(x, clusters, nc):
    sigma_R = 0.0
    for i in range(nc):
        sigma_R = sigma_R + compute_Di(i, x, clusters, nc)
    DB_index = float(sigma_R)/float(nc)
    return DB_index
```



## K-means

常规的K-means存在空簇的情况



**二分K-均值（Bisecting K-Means）**算法简介
  二分K-均值（Bisecting K-Means，以下简称BK-Means）是K-Means算法的衍生，他采取的划分策略是，从原始数据集开始，每次只将一个簇“一分为二”，直至产生期望的簇个数$k$为止，这种划分方法降低了K-Means算法趋于局部最优的风险，因此不会受到选取初始聚类中心带来的影响。具体的算法介绍如下：

## 指标 

[知乎链接-指标](https://zhuanlan.zhihu.com/p/572337932)

### 内部指标

> DB指数(Davies-Bouldin Index, 简称DBI)
>
> Dunn指数(Dunn Index, 简称DI)
>
> 轮廓系数(Silhouette Coefficient, 简称SC)
>
> 簇内误差平方和 （within-cluster sum of square error, 简称SSE)

#### DBI

Davies-Bouldin指数（DBI）（由大卫L·戴维斯和唐纳德·Bouldin提出）**是一种评估度量的聚类算法**。

> 假设有一堆数据点，把它们氛围n个簇，公式如：
>
> 通过分散程度、中心距离、相似度计算DBI指数，优化$k$值的选择



**分散程度**

> DBI定义了一个分散的值$S_i$，度量第$i$个类中数据点的**分散程度**

$$
s_i=\{\frac{1}{T_i}\sum^{T_i}_{j}\vert X^{(i)}_{j} - A^{(i)}\vert^q\}^{\frac{1}{q}}
$$

$$X_j$$表示第$$i$$个簇中第$$j$$个数据点，$$A_j$$表示第$i$簇的中心，$$T_i$$表示第$i$类中数据点的个数，$$q$$取1表示个点到各中心点距离的均值，$$q$$取2表示各点到中心距离的标准差。



**中心距离**

> DBI定义了一个距离值$M_{i,j}$，表示第$$i$$类与第$$j$$类中心的距离

$$
M_{i,j}=\{\sum^N_{k=1}\vert a_{ki}-a_{kj}\vert^p\}^{\frac{1}{p}}
$$

$$a_{k,i}$$表示第$i$质心个点的第$k$个属性的值



**相似度**

> DBI定义了一个相似度的值$R_{i,j}$

$$
R_{i,j}=\frac{S_i+S_j}{M_{i,j}}
$$

衡量两个簇的相似度



通过上面的公式，从$R_{i,j}$中选出最大值$\hat R_i=\max(R_{i,j})$，即第$i$类与其他类相似度中最大的相似度的值
$$
\overline R=\frac{1}{N}\sum^N_{i=1}R_i
$$
最后计算每个类的最大相似度的均值，得到DBI指数，当$$\overline R$$值越小时，分类效果越好。



**总的来说，这个DBI就是计算类内距离之和与类间距离之比，来优化k值的选择，避免K-means算法中由于只计算目标函数Wn而导致局部最优的情况。**



### 外部指标

> Jaccard系数(Jaccard Coefficient, 简称JC)
>
> FM指数(Fowlkes and Mallows Index, 简称FMI)
>
> Rand指数(Rand Index, 简称RI)





