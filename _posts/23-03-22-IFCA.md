---
layout: distill
title: An Efficient Framework for Clustered Federated Learning 阅读
description: 个性化模型学习,通过经验损失进行聚类
date: 2023-03-24
tags: Papers
categories: Papers-Read

authors:
  - name: Avishek Ghosh
    url: "https://sites.google.com/view/avishekghosh/home"
    affiliations:
      name: Centre for Machine Intelligence and Data Science, Indian Institute of Technology
  - name: Jichan Chung
    url: "https://scholar.google.com/citations?user=pXQfWTkAAAAJ&hl=en"
    affiliations:
      name: Dept of EECS, UC Berkeley
  - name: Dong Yin
    url: "https://scholar.google.com/citations?user=YtM8P88AAAAJ&hl=en"
    affiliations:
      name: DeepMind


bibliography: 2023-03-22-IFCA.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Self Comprehension
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Related work
  - name: Problem formulation
  - name: Algorithm
  - name: Theoretical guarantees
  - name: Experiments
  - name: Broader Impact
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
## Self Comprehension

作者将聚类任务放在客户端上，而不是服务器上(一次聚类)

> 有点降低了服务器的计算，缺点时客户端需要上传自己所属的集群

## Introduction

提出迭代联邦聚类算法(IFCA)，交替估计用户的集群身份并通过梯度下降算法优化用户集群的模型参数。

> 在具有平方损失的线性模型中分析算法的收敛速度，分析通用的强凸函数和平滑损失函数

## Related work

**从非 i.i.d. 中学习单一全局模型：**[49、34、21、37、24、30]。[38、36、8]

```markdown
MOCHA，考虑多任务学习设置，形成一个确定性优化问题，其中用户的相关矩阵是正则化项，
```

**通过全局模型微调进行个性化：**

```markdown
数据作为元学习问题 [4, 15, 8]。在此设置中，目标是首先获得一个全局模型，然后每个设备使用其本地数据微调模型。该公式的基本假设是不同用户之间的数据分布相似，全局模型可以作为一个很好的初始化
```

\[36\]\[10\]考虑了FL的制定，两项工作均采用集中式的聚类算法，如K-means

> 其中服务器必须识别所有用户的集群身份，导致中心计算成本过高，这些算法可能不适用于深度神经网络等大型模型或具有大量模型的应用程序。

**IFCA**

潜在变量问题，作者提出的公式可以被视为分布式设置中具有潜在变量的统计估计问题

> 而潜在变量是统计学和非凸优化中的经典话题，如高斯混合模型GMM[44、20]，线性回归混合模型[7、43、48]和相位检索[9,29]

> 解决这些问题的两种流行方法是：期望最大化(EM)和交替最小化（AM）

近年来EM和AM在集中式环境中的融合方面的理解取得了一些进展[31, 5, 47, 1, 42]

> 如从一个合适的点开始，它们的收敛速度会很快，有时它们会有超线性的收敛速度[44, 11]

## Problem formulation

集群划分：$m$台机器划分为$k$个不相交的集群，假设不知道每台机器所属的集群

集群内数据点：假设每个客户端$i\in S^*_j$包含数据集$\mathcal D_j$的$n$个$i.i.d$数据点$\{z^{i,1},z^{i,2},...,z^{i,n}\}$，

> 每个数据点$z^{i,j}$由特征和相应的标签组成$z^{i,\ell}=(x^{i,\ell},y^{i,\ell})$

让$f(\theta;z)：\Theta\rightarrow\mathbb R$表示与数据点$z$相关的损失函数，$\Theta\subseteq \mathbb R^d$是参数空间【作者选择$\Theta= \mathbb R^d$】

作者的目标是最小化对所有$j\in[k]$的总体损失$F^j(\theta):=\mathbb E_{z\sim\mathcal D_j}[f(\theta;z)]$

> 拓展：这里的$E_{z\sim\mathcal D_j}[f(\theta;z)]$表示函数$f$关于样本空间$\mathcal D_j$的期望，指的是最大似然估计(Maximum Likelihood Estimation)，在数据集合$\mathcal D_j$已知的前提下， 对于函数$f$的参数$\theta$的极大似然估计值。
>
> 换句话说，希望可以得到最小化期望时的$\theta$值

试图找到$\{\hat \theta\}^k_{j=1}$使得$\theta^*_j=\text {argmin}_{\theta\in\Theta}F^j(\theta),j\in[k]$

原文：”因为只有有限数据，所以利用了经验损失函数“

> 不太理解这句话，如果不是因为这个还有其他的可选么

$Z\subseteq\{z^{i,1},z^{i,2},...,z^{i,n}\}$是第$i$台机器上数据点的子集

> 或者说：第$i$台机器上的数据点$Z\subseteq\{z^{i,1},z^{i,2},...,z^{i,n}\}$是子集?

定义经验损失为$F_i(\theta;Z)=\frac{1}{\vert Z\vert}\sum_{z\in Z}f(\theta;z)$，用$F_i(\theta)$来表示第$i$-th worker的经验损失

## Algorithm

算法主要思想：估计集群身份时交替使用最小化损失函数，讨论了IFCA的两种变体，即梯度平均和模型平均

<img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230322165524883.png" alt="image-20230322165524883" style="zoom:50%;" />

<img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230322165302302.png" alt="image-20230322165302302" style="zoom: 25%;" />

算法从$k$个模型参数参数开始，$\theta^{(0)}_j,j\in[k]$。在第$t$-th迭代中，IFCA的中心服务器随机选择客户端的子集，$M_t\in[m]$并广播当前模型参数$\{\theta_j^{(t)}\}_{j=1}^k$给$M_t$中的用户（称$M_t$为参与了设备的集合）

**通过参数评估**：每个客户端都配备了局部经验损失函数$F(\cdot)$使用收到的参数评估$F_i$，第$i$-th个客户端($i\in M_t$)==通过寻找最低损失的模型参数来估计其所在的集群标识==，如$\hat j=\text{argmin}_{j\in[k]}F_i(\theta^{(t)}_j)$

**通过梯度平均**：如果选择梯度平均选项，客户端讲计算参数$\theta^{(t)}_{\hat j}$的局部经验损失$F_i$的随机梯度，并将其集群身份估计和梯度法送回中心机器。

> $m$是所有客户端、$M_t$是随机挑选的部分客户端(子集)、$k$是集群个数、$\theta_j^{(0)}$是某一个集群的全局模型参数
>
> 【理解】：使用损失的多少来作为分类标准（如普通k-means的欧氏距离）

中心服务器接收到所有客户端的梯度和集群身份估计，收集集群身份估计相同的客户端，对各相应集群中的模型参数进行梯度下降更新，如果选择平均选项(类似于联邦平均算法[27])，每个参与的设备需要运行$\tau$次本地随机梯度下降更新，并且发送新模型及其集群对服务器的身份估计。然后中心服务器对集群身份估计相同的客户端新模型进行平均

### 具体实施（多任务学习权重共享）

集群结构可能是模糊的，这意味着尽管来自不同集群的数据分布不同，但模型应该利用所有用户数据的一些共同属性。

> 基于此，使用"多任务学习权重共享技术[3]"

具体而言，训练神经网络模型时，在所有集群之间共享前几层的权重，这样可以通过可获得的数据学习到良好的表示。

然后仅在最后（或最后几层上）运行IFCA算法以解决不同集群中相应的不同的分布。

使用算法1中与$\theta_j^{(t)}$相符的子集运行IFCA，对剩余的运行联邦平均或vanilla梯度平均

这样的好处是中心服务器不需要发送$k$个模型给所有机器，只需要发送所有权重的一个子集，它具有$k$个不同版本，以及一个共享的副本层

当集群身份的估计在几次并行迭代中没有改变认为稳定

## Theoretical guarantees

考虑具有梯度平均的IFCA

## 实验

不会再每次迭代时重新采样新数据点，此外，还可以放宽初始化要求

> More specifically，对于线性模型，我们观察到随机初始化和几次重新启动足以确保算法1的手链

<img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230323102018587.png" alt="image-20230323102018587" style="zoom:33%;" />

> 不同成功的概率:
>
> - (a)、(b)是分离尺度$R$和加性噪声$\sigma$
> - (c)、(d)是客户端数量$m$和每个客户端采样大小$n$
>
> (a)和(b)中，随着$R$的增加成功的概率增加，即，基本反映出真实参数向量的之间更多的分离
>
> (c)和(d)中，随着$mn$的增加成功的概率在提升，即每个客户端的数据更多/客户端的数量更多，成功的概率也会提升

首先在具有平方损失的线性模型上使用梯度平均(选项I)评估算法,

首先生成$\theta^*_j\sim\text{Bernoulli(0.5)}的值，并且将它们的$$\ell_2$范数调整为$R$，这确保了$\theta^*_j$之间的间距与$R$成正比期望

每次实验中，首先生成参数向量$\theta^*_j$并固定他们，根据独立的伯努利分布对应的随机初始化$\theta^{(0)}_j$

运行算法1，300次迭代，步长不变。对于$k=2$和$k=4$，分别在$\{0.01,0.1,1\}$和$\{0.5,1.0,2\}$中选择步长。为了确定是否成功学习了模型，我们回到上述步长并定义距离的度量：$\text{dist}=\frac{1}{k}\sum^k_j=1\Vert \hat \theta_j-\theta^*_j\Vert$，其中$\{\hat \theta_j\}^k_{j=1}$是从算法1中获得的参数估计，如果对于$\theta^*_j$的固定集合，在10个随机初始化参数$\theta_j^{(0)}$中，至少在一个场景中获得$\text {dist}\le0.6\sigma$，则实验称为成功

在图2(a-b)中，针对分离参数$R$绘制了40次实验的经验成功概率。将问题参数设置为

1. (a),  $k=2并且(m,n,d)=(100,100,1000)$
2. (b),  $k=4并且(m,n,d)=(400,100,1000)$

正如所看到的，当$R$变大时，即参数之间的距离会变大，问题变得更容易解决，成功概率更高。这验证了作者的理论结果，更高的信噪比产生更小的误差层。

在图2(c-d)中，作者描述了对$m,n$的依赖性，将$R$和$d$固定为

1. (c),  $(R,d)=(0.1,1000)$
2. (d),  $(R,d)=(0.5,1000)$

观察到，当增加$m$或n$的数量时，成功的概率会提高

### MNIST和CIFAR的旋转

基于MNIST[19]和CIFAR-10[18]数据集创建了聚类FL数据集。为模拟不同机器上的数据从不同分布生成的环境，使用==旋转扩充数据集==

并创建旋转MNIST[25]和旋转CIFAR数据集

> 对MNIST应用0、90、180、270度旋转来扩充数据集，从而产生$k=4$个簇，对于给定的$m和n$满足$mn=60000k$，将图像随机划分为$m$个客户端，每个客户端上有$n$个具有相同旋转的图像。同样用相同的方式拆分测试数据集$m_{test}=10000k/n$个客户端
>
> 对于CIFAR数据集，与MNIST相似操作，拆分主要区别在于创建了$k=2$个具有0度和190度旋转的簇，

作者注意到，通过操纵MNIST和CIFAR-10等标准数据集来创建不同的任务已在持续学习研究社区中得到广泛采用[12、16、25]。对于集群FL，使用旋转创建数据集有助于我们模拟具有清晰集群结构的联邦学习设置。

对于MNIST实验，使用具有ReLU激活的券链接神经网络，单个隐藏层大小为200，CIFAR实验，使用2个卷积层和2个全连接层组成的卷机神经网络模型，图像通过标准数据增强（翻转、随机裁剪）进行预处理

作者将IFCA算法与两种基线算法进行比较，即全局模型和局部模型方案。对于IFCA，我们使用模型平均(算法1中的选项II)/

> 对于MNIST实验，我们使用完整的客户端参与(对所有$t$，$M_t=[m]$)。对于算法1的本地更新，我们选择$\tau=10$并且补偿$\gamma=0.1$。对于CIFAR实验，选择$\vert M_t\vert=0.1m$，并且应用下降的补偿0.99，还为LocalUpdate的过程设置$\tau=5$，batch size 50
>
> 遵循之前的工作[28]（fedAvg）
>
> 1. 在全局模型方案中，该算法尝试学习一个单一的全局模型，该模型可以从所有分布中进行预测。该算法不考虑聚类身份，因此算法1中的模型平均的操作变成$\theta^{(t+1)}=\sum_{i\in M_t}\hat \theta/\vert M_t\vert$，即对所有参与机器参数进行平均。
> 2. 在局部模型的方案中每个解ID那种的模型仅对局部可用数据进行梯度下降，不进行模型平均

对于IFCA和全局方案，通过以下方式进行推理：

> 每台测试客户端，我们对所有学习模型(IFCA的$k$个模型和一个全局模型模型)进行推理，并从产生最小损失的模型计算准确率。
>
> 为了测试局部模型基线，在相同分布的测试数据测试准确率(如那些旋转的数据)

作者展示了客户端中所有模型的平均准确度，对于所有算法，使用5个不同的随机种子进行实验并报告平均值和标准偏差

实验结果如表1所示，可以观察到作者的算法比两个基线性能更好，当运行IFCA算法时，作者观察到可以逐渐找到工作机器底层集群的标识，并且在找到正确的集群后，使用具有相同分布的数据训练和测试每个模型，从而获得更好的准确性。全局基线模型的性能比作者提出算法的性能更差。

<img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230324104723562.png" alt="image-20230324104723562" style="zoom:50%;" />

### Fedrated Emnist

作者在Federated Emnist(FEMNIST)[2]上提供了额外的实验结果，使用4.1节中提到的权重共享技术。

使用具有两个卷机层+一个最大池化层+两个全连接层的神经网络，共享所有层的权重，除了IFCA训练的最后一层。将簇的数量$k$视为超参数，并使用不同的$k$进行实验，对比了具有IFCA的全局模型和局部模型的方法，并且还与一次性集中聚类算法进行了比较。测试精度如表2其中计算了5次独立运行的平均值和标准偏差。如所看到的，IFCA比全局模型和局部模型方法显出明显的优势，

> IFCA和One-shot算法的结果是相似的，但是如第2节中强调的，IFCA不运行集中式的聚类程序，因此降低了服务器的计算成本。
>
> 最后，作者观察到IFC对于簇数$k$的选择是稳健的。$k=2$和$k=3$的算法结果类似，并且注意到当$k>3$时，IFCA自动识别出3个簇，其余簇为空这表明IFCA在聚类结构不明确切簇类数量未知的现实问题中的适用性

<img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230324104741100.png" alt="image-20230324104741100" style="zoom:50%;" />

## Broader Impact

作者提出它们的框架将更好地保护联邦学习系统中用户的隐私，同时仍提供个性化预测，

> 不需要用户将自己的任何个人数据发送到中央服务器，用户仍然可以使用服务器的计算能力学习个性化模型
>
> 一个潜在的风险是作者的算法仍然需要用户将集群身份发送到中央服务器。
