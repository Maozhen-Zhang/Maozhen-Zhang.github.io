<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://maozhenzhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://maozhenzhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-12T11:34:03+00:00</updated><id>https://maozhenzhang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">PPML-1章1节-多项式曲线拟合</title><link href="https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A01%E8%8A%82-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88/" rel="alternate" type="text/html" title="PPML-1章1节-多项式曲线拟合"/><published>2023-04-17T13:11:00+00:00</published><updated>2023-04-17T13:11:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A01%E8%8A%82-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A01%E8%8A%82-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88/"><![CDATA[<h1 id="多项式曲线拟合">多项式曲线拟合</h1> <p>略…</p> <h2 id="6惩罚项">6.惩罚项</h2> <p>模型参数过多时，学习到的参数会有很大的绝对值，</p> <blockquote> <p>理解：</p> <p>因为目标函数最小化 \(\widetilde E(\boldsymbol w)=\sum_{n=1}^N\vert y(x_n,\boldsymbol w)-t_n \vert^2\) （凸优化原理么）为了使$\widetilde E$最小化，会使得$w$极大，因此加上惩罚项\(\lambda/2\Vert \boldsymbol w\Vert^2\)，使得： \(\widetilde E(\boldsymbol w)=\frac{1}{2}\sum_{n=1}^N\vert y(x_n,\boldsymbol w)-t_n \vert^2+\lambda/2\Vert \boldsymbol w\Vert^2\) 公式变为两部分最小化，后项最小约束为：\(\lambda/2\Vert \boldsymbol w\Vert^2=0\)与前项互相制约。</p> <p>系数 λ 控制了正则化项相对于平方和误差项的重要性。</p> </blockquote>]]></content><author><name></name></author><category term="PPML(一)"/><category term="PPML"/><summary type="html"><![CDATA[PPML阅读，第一章第一节，多项式曲线拟合]]></summary></entry><entry><title type="html">PPML-1章2节-概率论</title><link href="https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A02%E8%8A%82-%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="alternate" type="text/html" title="PPML-1章2节-概率论"/><published>2023-04-17T13:11:00+00:00</published><updated>2023-04-17T13:11:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A02%E8%8A%82-%E6%A6%82%E7%8E%87%E8%AE%BA</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A02%E8%8A%82-%E6%A6%82%E7%8E%87%E8%AE%BA/"><![CDATA[<h1 id="概率论">概率论</h1> <h2 id="来源">来源</h2> <p><a href="https://zhuanlan.zhihu.com/p/430297225">文章地址</a></p> <p>自己阅读..待定</p> <p>仅存我认为了解较模糊的信息。</p> <h2 id="概率论计算规则">概率论计算规则</h2> <p>加和规则：\(P(X)=\sum_Y P(X,Y)\)</p> <table> <tbody> <tr> <td>乘积规则：$$P(X,Y)=P(Y</td> <td>X)P(X)$$</td> </tr> </tbody> </table> <blockquote> <table> <tbody> <tr> <td>\(P(X,Y)\)又称作联合概率，\(P(X)\)和\(P(Y)\)又被称为边缘概率，$P(Y</td> <td>X)$被称为条件概率</td> </tr> </tbody> </table> </blockquote> <table> <tbody> <tr> <td>全概率公式：$$P(X)=\sum_Y P(Y</td> <td>X)P(X)$$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>贝叶斯公式：$$P(Y</td> <td>X)=\frac{P(X</td> <td>Y)P(Y)}{P(X)}$$</td> </tr> </tbody> </table> <blockquote> <table> <tbody> <tr> <td>其中$$P(X</td> <td>Y)\(被称为似然；\)P(Y)\(被称为先验；\)P(X)\(被称为归一化常数，被称为常数是因为这时研究的变量为\)Y\(，无论\)Y\(取什么值，\)P(X)\(的值都不会变，相对于\)Y\(而言，\)P(X)\(是常数；\)P(Y</td> <td>X)$$被称为后验。</td> </tr> </tbody> </table> </blockquote>]]></content><author><name></name></author><category term="PPML(一)"/><category term="PPML"/><summary type="html"><![CDATA[PPML阅读，第一章第二节，概率论部分]]></summary></entry><entry><title type="html">PPML-1章5节-决策论</title><link href="https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A05%E8%8A%82-%E5%86%B3%E7%AD%96%E8%AE%BA/" rel="alternate" type="text/html" title="PPML-1章5节-决策论"/><published>2023-04-17T13:11:00+00:00</published><updated>2023-04-17T13:11:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A05%E8%8A%82-%E5%86%B3%E7%AD%96%E8%AE%BA</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/PPML-1%E7%AB%A05%E8%8A%82-%E5%86%B3%E7%AD%96%E8%AE%BA/"><![CDATA[<h1 id="决策论">决策论</h1> <h2 id="示例">示例</h2> <p>\(C_1\)代表患有癌症，\(C_2\)代表不患有癌症，\(\boldsymbol x\)表示某人的X光片，根据以往的病例训练一个模型，对新的\(x\)进行预测。 \(p(C_k|\boldsymbol x)=\frac{p(\boldsymbol x|C_k)p(C_k)}{p(\boldsymbol x)}\) 推断问题根据训练数据得出\(p(\boldsymbol x,C_k)\)，进而得到\(p(\boldsymbol x|C_k)\)和\(p(C_k)\)，从而根据上面的贝叶斯公式计算出\(p(C_k|\boldsymbol x)\)</p> <blockquote> <table> <tbody> <tr> <td>根据推断的概率分布给出$x$对应的类别，$$p(C_k</td> <td>\boldsymbol x)\(为后验，\)P(C_k)$$为先验</td> </tr> </tbody> </table> <p>即在没有\(x\)出现前，某人患有癌症或健康的概率。</p> </blockquote> <p>待续…</p> <hr/> <h2 id="推断和决策">推断和决策</h2> <p>判别模型：</p> <table> <tbody> <tr> <td>对于每个类别\(C_k\)，独立确定条件密度$$p(x</td> <td>C_k)\(，然后推断先验概率\)p(C_k)\(，之后使用贝叶斯定理\)p(C_k</td> <td>\boldsymbol x)=\frac{p(\boldsymbol x</td> <td>C_k)p(C_k)}{p(\boldsymbol x)}$$，求出后验概率</td> </tr> </tbody> </table> <blockquote> <p>这是一个推断问题，得到的是关于这个\(\boldsymbol x\)对于\(C_i,i\in\{1,2,...,k\}\)的不同概率</p> <p>对于每个类别，计算每个类别下的概率密度模型，推断新的参数\(x\)</p> </blockquote> <p>生成模型：</p> <table> <tbody> <tr> <td>直接对\(p(x,C_k)\)进行建模，然后归一化\(\sum_k p(x,C_k)=p(x)\)，然后通过$$p(C_k</td> <td>x)=\frac{p(x,C_k)}{p(x)}$$得到后验概率。</td> </tr> </tbody> </table> <blockquote> <p>假设符合一定的分布规律，生成整个模型的概率分布函数</p> </blockquote> <font color="red">【疑惑】</font>]]></content><author><name></name></author><category term="PPML(一)"/><category term="PPML"/><summary type="html"><![CDATA[PPML阅读，第一章第五节，决策论]]></summary></entry><entry><title type="html">PPML(零)-开章</title><link href="https://maozhenzhang.github.io/blog/2023/PPML/" rel="alternate" type="text/html" title="PPML(零)-开章"/><published>2023-04-17T13:11:00+00:00</published><updated>2023-04-17T13:11:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/PPML</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/PPML/"><![CDATA[<h1 id="ppml">PPML</h1> <p>开局，占个坑，今天看到了一个人的知乎帖子，看经历感觉应该是博士在读，有些..被刺激到了，</p> <p>也一直想要读这本书，奈何英语不好，加上行动力不足，</p> <p>先开贴占个坑，将来补上，</p> <p>也根据别人的笔记学习学习</p> <p>别人更的文章地址：<a href="https://zhuanlan.zhihu.com/p/430290722">知乎笔记</a></p>]]></content><author><name></name></author><category term="PPML"/><category term="PPML"/><summary type="html"><![CDATA[PPML阅读]]></summary></entry><entry><title type="html">差分隐私的算法基础(3章5节)-组合定理</title><link href="https://maozhenzhang.github.io/blog/2023/The-Algorithmic-Foundationsof-Differential-Privacy-3%E7%AB%A05%E8%8A%82-%E7%BB%84%E5%90%88%E5%AE%9A%E7%90%86/" rel="alternate" type="text/html" title="差分隐私的算法基础(3章5节)-组合定理"/><published>2023-04-17T13:11:00+00:00</published><updated>2023-04-17T13:11:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/The%20Algorithmic%20Foundationsof%20Differential%20Privacy-3%E7%AB%A05%E8%8A%82-%E7%BB%84%E5%90%88%E5%AE%9A%E7%90%86</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/The-Algorithmic-Foundationsof-Differential-Privacy-3%E7%AB%A05%E8%8A%82-%E7%BB%84%E5%90%88%E5%AE%9A%E7%90%86/"><![CDATA[<h1 id="组合定理">组合定理</h1> <p><strong>定理3.13：</strong></p> <table> <tbody> <tr> <td>设$\mathcal M_1:\mathbb N^{</td> <td>\chi</td> <td>}\rightarrow\mathcal R_1$是一个满足$\epsilon_1$-DP的算法，$\mathcal M_2:\mathbb N^{</td> <td>\chi</td> <td>}\rightarrow\mathcal R_2$是一个满足$\epsilon_2$-DP的算法，那么它们的组合定义为$\mathcal M_{1,2}:\mathbb N^{</td> <td>\chi</td> <td>}\rightarrow\mathcal R_1\times\mathcal R_2$，则$\mathcal M_{1,2}(x)=(\mathcal M_1(x),\mathcal M_2(x))$满足$\epsilon_1+\epsilon_2$-DP</td> </tr> </tbody> </table> <blockquote> <p>证明：</p> <p>设$x,y\in\mathbb N^{|\chi|}$满足$\Vert x - y\Vert_1\le1$，对于任意的$(r_1,r_2)\in\mathcal R_1\times\mathcal R_2$，则： \(\begin{aligned} \frac{\operatorname{Pr}\left[\mathcal{M}_{1,2}(x)=\left(r_1, r_2\right)\right]}{\operatorname{Pr}\left[\mathcal{M}_{1,2}(y)=\left(r_1, r_2\right)\right]} &amp; =\frac{\operatorname{Pr}\left[\mathcal{M}_1(x)=r_1\right] \operatorname{Pr}\left[\mathcal{M}_2(x)=r_2\right]}{\operatorname{Pr}\left[\mathcal{M}_1(y)=r_1\right] \operatorname{Pr}\left[\mathcal{M}_2(y)=r_2\right]} \\ &amp; =\frac{\operatorname{Pr}\left[\mathcal{M}_1(x)=r_1\right]}{\operatorname{Pr}\left[\mathcal{M}_1(y)=r_1\right]} \frac{\operatorname{Pr}\left[\mathcal{M}_2(x)=r_2\right]}{\operatorname{Pr}\left[\mathcal{M}_2(y)=r_2\right]} \\ &amp; \leq \exp \left(\varepsilon_1\right) \exp \left(\varepsilon_2\right) \\ &amp; =\exp \left(\varepsilon_1+\varepsilon_2\right) \end{aligned}\) 也有$\frac{\operatorname{Pr}\left[\mathcal{M}<em>{1,2}(x)=\left(r_1, r_2\right)\right]}{\operatorname{Pr}\left[\mathcal{M}</em>{1,2}(y)=\left(r_1, r_2\right)\right]}\ge\exp(-(\epsilon_1+\epsilon_2))$</p> </blockquote> <font color="red">思考:</font> <p>值域是$\mathcal R_1\times \mathcal R_2$，所以说它们是两个算法想称，也就是输入为$x$时，落入不同区域的概率(分布可能性)，因此有：</p> <blockquote> <p>$\Pr[\mathcal M_1(x)=r_1]$，满足差分隐私的算法$\mathcal M_1$中输入$x$，其值落到$r_1$的概率。</p> <p>$\mathcal M_{1,2}(x)=(\mathcal M_1(x),\mathcal M_2(x))$ ，表示对应的两个输出，落到的总概率区间（相乘）</p> </blockquote> <table> <tbody> <tr> <td><strong>推论</strong>：设$\mathcal M_i:\mathbb N^{</td> <td>\chi</td> <td>}\rightarrow\mathcal R_i$满足$(\epsilon_i,0)$-DP，$i\in[k]$。如果$\mathcal M_{[k]}:\mathbb N^{</td> <td>\chi</td> <td>}\rightarrow \prod_{i=1}^k\mathcal R_i$被定义为$\mathcal M_{[k]}(x)=(\mathcal M_1(x),…,\mathcal M_k(x))$，则$M_{[k]}(x)$满足$(\sum_{i=1}^k\epsilon_i,0)$-DP。</td> </tr> </tbody> </table> <p><strong>定理3.14：</strong></p>]]></content><author><name></name></author><category term="TAFDP(三)"/><category term="TAFDP"/><summary type="html"><![CDATA[The Algorithmic Foundationsof Differential Privacy第3章第五节组合定理理解]]></summary></entry><entry><title type="html">newnewnewtest</title><link href="https://maozhenzhang.github.io/blog/2023/distill/" rel="alternate" type="text/html" title="newnewnewtest"/><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/distill</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/distill/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like</p> \[E = mc^2\] <p>.</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <hr/> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <code class="language-plaintext highlighter-rouge">&lt;d-cite key="gregor2015draw"&gt;&lt;/d-cite&gt;</code> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code>This will become a hoverable footnote.<code class="language-plaintext highlighter-rouge">&lt;/d-footnote&gt;</code></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags. An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>. For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode. You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li> <p>Unordered list can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I'm an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I'm an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I'm a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">Conda</title><link href="https://maozhenzhang.github.io/blog/2023/Conda/" rel="alternate" type="text/html" title="Conda"/><published>2023-04-14T12:52:00+00:00</published><updated>2023-04-14T12:52:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/Conda</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/Conda/"><![CDATA[<p>[TOC]</p> <h1 id="conda">Conda</h1> <p>作为一个虚拟环境配置管理软件，可以在电脑自身环境外创造虚拟环境(base是自身创造的虚拟环境)</p> <p>在每个虚拟环境中就会相当于一个电脑（如虚拟机）</p> <p>Pycharm等IDE开发工具可以选择conda中的某一个虚拟环境使用，进行的操作（更新、安装、删除）都会直接对虚拟环境进行操作</p> <p>可再Pycharm中的setting中查看虚拟环境的包，同<code class="language-plaintext highlighter-rouge">conda list</code></p> <h2 id="conda-1">Conda</h2> <h2 id="查看conda信息">查看conda信息</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#查看conda信息</span>
conda info
<span class="c">#查看配置信息</span>
conda config <span class="nt">--show</span>
</code></pre></div></div> <h3 id="查看当前系统环境">查看当前系统环境</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda info <span class="nt">-e</span>
conda <span class="nb">env </span>list
</code></pre></div></div> <h3 id="创建新的环境">创建新的环境</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 指定python版本为2.7，注意至少需要指定python版本或者要安装的包</span>
<span class="c"># 自动安装python2.7最新版本</span>
conda create <span class="nt">-n</span> env_name <span class="nv">python</span><span class="o">=</span>2.7
<span class="c"># 同时安装必要的包</span>
conda create <span class="nt">-n</span> env_name numpy matplotlib <span class="nv">python</span><span class="o">=</span>2.7
</code></pre></div></div> <h3 id="删除环境">删除环境</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda remove <span class="nt">-n</span> envs_name <span class="nt">--all</span>
</code></pre></div></div> <h3 id="克隆环境">克隆环境</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n new_env1 new_env
conda create -n env_enviroments --clone env_enviroments
</code></pre></div></div> <h3 id="环境切换">环境切换</h3> <pre><code class="language-Shell"># 切换到新环境
# linux/Mac下需要使用source activate env_name
conda activate env_name
#退出环境，也可以使用`activate root`切回root环境
conda deactivate env_name
</code></pre> <h3 id="安装包">安装包</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#制定版本</span>
conda <span class="nb">install </span><span class="nv">pytorch</span><span class="o">=</span>1.2.0 torchvision torchaudio <span class="nt">-c</span> pytorch
<span class="c">#这将安装由 PyTorch 官方和 conda-forge 社区维护的支持 GPU 的 PyTorch 版本。</span>
conda <span class="nb">install </span>pytorch torchvision torchaudio <span class="nt">-c</span> pytorch <span class="nt">-c</span> conda-forge
</code></pre></div></div> <h3 id="查看包">查看包</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#当前环境所有的包</span>
conda list
<span class="c">#某一个包的信息</span>
conda list &lt;package_name&gt;

<span class="c">#查看conda源路径下所有指定包的信息</span>
conda search pytorch
conda search &lt;package_name&gt;
</code></pre></div></div> <h2 id="包更新命令">包更新命令</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#</span>
conda update python

<span class="c">#该方式不太行</span>
conda update pytorch torchvision
<span class="c">#使用该方式</span>
conda <span class="nb">install </span>pytorch torchvision torchaudio <span class="nt">-c</span> pytorch
</code></pre></div></div> <h3 id="删除包">删除包</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#删除pytorch包</span>
conda remove pytorch torchvision torchaudio
<span class="c">#安装支持 GPU 的 PyTorch 版本：您可以使用以下命令通过 conda 安装支持 GPU 的 PyTorch 版本</span>
<span class="c">#这将安装由 PyTorch 官方和 conda-forge 社区维护的支持 GPU 的 PyTorch 版本。</span>
conda <span class="nb">install </span>pytorch torchvision torchaudio <span class="nt">-c</span> pytorch <span class="nt">-c</span> conda-forge

</code></pre></div></div> <h2 id="镜像源配置">镜像源配置</h2> <h3 id="查看镜像源">查看镜像源</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#查看conda信息</span>
conda info
<span class="c">#查看配置信息</span>
conda config <span class="nt">--show</span>
<span class="c">#查看镜像源信息</span>
conda config <span class="nt">--show</span> channel
</code></pre></div></div> <h3 id="修改源源镜像文件">修改源源镜像文件</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#修改源镜像文件</span>
conda config <span class="nt">--edit</span>
</code></pre></div></div> <p>查看配置项<code class="language-plaintext highlighter-rouge">channels</code>，如果显示带有<code class="language-plaintext highlighter-rouge">tsinghua</code>,则说明已经安装过清华镜像</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>channels:
- https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/
- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
</code></pre></div></div> <h3 id="修改镜像源">修改镜像源</h3> <p>添加</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda config <span class="nt">--add</span> channels url
<span class="c">#添加中科大镜像</span>
conda config <span class="nt">--add</span> channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
</code></pre></div></div> <p>删除</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#删除url镜像源</span>
conda config <span class="nt">--remove</span> channels url
<span class="c">#如</span>
conda config <span class="nt">--remove</span> channels https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/
</code></pre></div></div> <p>设置搜索时显示通道地址</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda config <span class="nt">--set</span> show_channel_urls <span class="nb">yes</span>
</code></pre></div></div> <h3 id="镜像的设置">镜像的设置</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda config <span class="nt">--set</span> show_channel_urls <span class="nb">yes</span>  <span class="c"># 显示当前源的 URL</span>
conda config <span class="nt">--set</span> channel_priority strict  <span class="c"># 设置源的优先级为严格模式</span>
conda config <span class="nt">--set</span> channels &lt;channel_name&gt;  <span class="c"># 设置新的源，例如：conda-forge</span>
</code></pre></div></div> <h2 id="应用的配置">应用的配置</h2> <h3 id="配置虚拟环境的jupyter">配置虚拟环境的jupyter</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>ipykernel
<span class="c">#设置jupyter的名称，jupyterName是显示在jupyter的名称</span>
python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> jupyterName
<span class="c">#执行打开jupyter notebook</span>
jupyter notebook
<span class="c">#如果发现jupyter notebook not found，则需要进行安装</span>
pip <span class="nb">install </span>jupyter notebook
</code></pre></div></div> <h2 id="问题">问题</h2> <h5 id="代码运行不报错直接结束排查发现在导入库名处终止">代码运行不报错，直接结束，排查发现在导入库名处终止</h5> <p>异常原因：</p> <blockquote> <p>导入的包或许需要升级，兼容性存在问题</p> </blockquote> <p>解决方法，升级/降级库版本</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> matplotlib
</code></pre></div></div> <h5 id="报错源路径下找不到current_repodatajson">报错源路径下找不到<code class="language-plaintext highlighter-rouge">current_repodata.json</code></h5> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 报错信息：</span>
PS C:<span class="se">\U</span>sers<span class="se">\M</span>z&gt; conda create <span class="nt">-n</span> env_python2 <span class="nv">python</span><span class="o">=</span>2.7
Collecting package metadata <span class="o">(</span>current_repodata.json<span class="o">)</span>: failed

CondaHTTPError: HTTP 000 CONNECTION FAILED <span class="k">for </span>url &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/win-64/current_repodata.json&gt;
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.
<span class="s1">'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/win-64'</span>
</code></pre></div></div> <p>更改方法：</p> <blockquote> <p>在进行常规操作的时候，比如创建一个虚拟环境，会报找不到<code class="language-plaintext highlighter-rouge">current_repodata.json</code>这个文件，这个时候，把路径<code class="language-plaintext highlighter-rouge">Anaconda3/Library/bin</code>目录下的<code class="language-plaintext highlighter-rouge">libcrypto-1_1-x64.dll</code>和<code class="language-plaintext highlighter-rouge">libssl-1_1-x64.dll</code>拷贝到<code class="language-plaintext highlighter-rouge">Anaconda3/DLLs</code>目录下即可。</p> </blockquote> <h1 id="pip">pip</h1> <h2 id="安装包-1">安装包</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">##带版本号</span>
pip <span class="nb">install </span><span class="nv">packageName</span><span class="o">==</span>xxx

<span class="c">##带路径安装</span>
pip <span class="nb">install </span><span class="nv">xxx</span><span class="o">=</span>xxx <span class="nt">-f</span>
pip <span class="nb">install </span><span class="nv">torch</span><span class="o">==</span>0.4.1 <span class="nt">-f</span> https://download.pytorch.org/whl/torch_stable.html
</code></pre></div></div> <h2 id="pip升级">pip升级</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
</code></pre></div></div> <h2 id="包升级">包升级</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> 包名
<span class="c">#如</span>
pip <span class="nb">install</span> <span class="nt">--upgrade</span> matplotlib
</code></pre></div></div> <p>#</p>]]></content><author><name></name></author><category term="Python-Package"/><category term="Linux"/><summary type="html"><![CDATA[anaconda3和pip包管理工具的语法记录]]></summary></entry><entry><title type="html">差分隐私(三)-隐私损失的度量</title><link href="https://maozhenzhang.github.io/blog/2023/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81(%E4%B8%89)-%E9%9A%90%E7%A7%81%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%BA%A6%E9%87%8F/" rel="alternate" type="text/html" title="差分隐私(三)-隐私损失的度量"/><published>2023-03-30T12:52:00+00:00</published><updated>2023-03-30T12:52:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81(%E4%B8%89)-%E9%9A%90%E7%A7%81%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%BA%A6%E9%87%8F</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81(%E4%B8%89)-%E9%9A%90%E7%A7%81%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%BA%A6%E9%87%8F/"><![CDATA[<h1 id="隐私损失">隐私损失</h1> <p>差分隐私中常用的隐私损失度量</p> <ol> <li>Renyi 散度：Renyi 散度是一种用于度量两个概率分布之间的差异的方法，可以用于计算差分隐私中的隐私损失。Renyi 散度在差分隐私中被广泛应用，它可以用于衡量隐私预算的耗尽情况，例如计算 epsilon 和 delta。</li> <li>Fisher 信息：Fisher 信息是一种度量随机变量关于参数的信息量的方法，可以用于计算差分隐私中的隐私损失。Fisher 信息在差分隐私中常常用于计算敏感性的上界，从而用于确定噪声的大小。</li> <li>Moments Accountant：Moments Accountant 是一种常用的差分隐私计算方法，用于估计累积隐私预算的消耗情况。Moments Accountant 基于矩的概念，通过计算梯度和噪声的高阶矩来估计隐私损失。</li> <li>高斯机制：高斯机制是一种常用的差分隐私机制，其中噪声服从高斯分布。可以通过计算高斯噪声的方差来估计差分隐私的损失。根据隐私参数的选择，可以计算 epsilon 和 delta。</li> </ol> <p>一些拓展方法：</p> <ol> <li>Moments Accountant with Sampling：这是 Moments Accountant 的一种改进方法，通过对数据集进行采样，从而减少计算高阶矩的开销，提高计算效率。</li> <li>Rényi Divergence with Rényi Privacy：这是一种使用 Rényi 散度来计算隐私损失的方法，其中 Rényi Privacy 是一种改进的隐私度量方式，可以根据具体的隐私需求来选择不同的 Rényi 参数。</li> <li>Pufferfish Privacy：这是一种新型的差分隐私计算方法，使用 Pufferfish Privacy 框架，通过对梯度向量进行量化和随机化，从而计算隐私损失。</li> <li>Zero-Concentrated Differential Privacy (ZCDP)：这是一种用于计算差分隐私损失的方法，特点是在计算梯度的高阶矩时，考虑了零集中差分隐私的特性，从而提高计算效率。</li> </ol> <p>选择的场景因素：</p> <ol> <li>隐私保护需求：不同的差分隐私度量方法适用于不同的隐私保护需求。例如，如果需要量化隐私泄露的严重性，并且关注个别记录的隐私保护，可以选择使用基于单个隐私泄露的度量方法，如最大化泄露概率 (maximal leakage) 或最大化后验概率 (maximal posterior probability)。而如果关注整体隐私保护效果，可以选择使用基于全局隐私泄露的度量方法，如全局灵敏度 (global sensitivity) 或隐私预算 (privacy budget)。</li> <li>数据集和模型属性：不同的数据集和模型属性可能对隐私度量方法的选择产生影响。例如，如果数据集的规模较大，可以选择计算代价较低的隐私度量方法，如 Moments Accountant with Sampling；如果数据集的规模较小，可以选择计算代价较高但更精确的隐私度量方法，如 Rényi Divergence with Rényi Privacy。另外，不同类型的模型，如深度神经网络、逻辑回归、决策树等，可能对隐私度量方法的适用性产生影响。</li> <li>计算效率和实际可行性：隐私度量方法的计算效率和实际可行性也是选择的考虑因素。一些隐私度量方法可能计算复杂度较高，需要较大的计算资源和时间，因此在实际应用中可能不太可行。在实际场景中，需要考虑计算效率和实际可行性，选择适合具体场景的隐私度量方法。</li> <li>具体应用场景：不同的应用场景可能对隐私度量方法的选择产生影响。例如，在医疗、金融等涉及敏感数据的领域中，对隐私保护要求较高，可能需要选择更为精确和严格的隐私度量方法；而在一些非敏感数据应用场景中，可以选择计算代价较低但足够保护隐私的度量方法。</li> </ol> <h1 id="composition-theorem">Composition Theorem</h1> <p>深度学习系统在经过一轮subsample，梯度计算，梯度裁剪，高斯加噪组成的训练后，得到了一组满足\((\epsilon,\delta)\)-DP的参数\(\theta_t\)，然后将\(\theta_t\)作为初始参数进行下一轮训练，经过\(T\)轮训练后，模型收敛。假设整个训练过程是公开的，即从第一到第\(T\)轮的所有模型参数都是可以获取的，那么怎么判断损失？</p> <p>Composition Theorem：计算整个训练系统的差分隐私损失。一个直觉是，一个由\(T\)个满足\((\epsilon,\delta)-DP\)的机制\(\mathcal M_t\)组成的队列系统\(\mathcal M\)的隐私损失最多是\((T\epsilon,T\delta)\)</p> <p>这个损失是否还能够再次减少，Strong Compposition theorem提出，\(T\)个机制Composition后，隐私损失变为\((\tilde \epsilon,\tilde \delta)\) \(\tilde{\epsilon}=\epsilon\sqrt{2T\ln(1/\delta')}+T\epsilon\frac{e^{\epsilon}-1}{e^{\epsilon}+1};\\\tilde{\delta}=T\delta+\delta'\) 一般取\(\delta^\prime=\delta\)，结合subsample定理，当\(\epsilon\rightarrow 0\)时，Strong Composition给出了\((\mathcal{O}(q\epsilon\sqrt{T\ln(1/\delta)},q(T+1)\delta)\)的隐私损失</p> <blockquote> <p>这个隐私损失与\(\delta\)相关，当\(\delta\)很小时，该损失变得非常大</p> <p>因此16年提出了Moments Account，</p> </blockquote> <p>Moments Account将深度学习的训练过程中Composition的隐私损失边界降低到\((q\epsilon\sqrt{T},\delta)\)</p> <blockquote> <p>该隐私损失边界的基本思想是将每一轮训练的隐私损失看成随机变量，将总隐私损失看成各轮随机变量的加和分布，通过计算随机变量的矩生成函数moment generating function，得到更精准的隐私界</p> <p>该方法最终可以归为RDP计算，在高斯机制下具有解析解</p> </blockquote>]]></content><author><name></name></author><category term="Differential-Privacy"/><category term="Privacy"/><summary type="html"><![CDATA[差分隐私隐私预算的度量]]></summary></entry><entry><title type="html">差分隐私(二)-联邦下的差分隐私类别</title><link href="https://maozhenzhang.github.io/blog/2023/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81(%E4%BA%8C)-%E8%81%94%E9%82%A6%E4%B8%8B%E7%9A%84%E7%9A%84%E7%B1%BB%E5%88%AB/" rel="alternate" type="text/html" title="差分隐私(二)-联邦下的差分隐私类别"/><published>2023-03-25T12:52:00+00:00</published><updated>2023-03-25T12:52:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81(%E4%BA%8C)-%E8%81%94%E9%82%A6%E4%B8%8B%E7%9A%84%E7%9A%84%E7%B1%BB%E5%88%AB</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81(%E4%BA%8C)-%E8%81%94%E9%82%A6%E4%B8%8B%E7%9A%84%E7%9A%84%E7%B1%BB%E5%88%AB/"><![CDATA[<h1 id="差分隐私的类别">差分隐私的类别</h1> <p>联邦场景下客户端级别的类型：</p> <ol> <li>样本级别（element-level）的差分隐私：训练的模型，不会泄露某个特定的样本是否参与了训练</li> <li>用户级别（client-level）的差分隐私：训练的模型，不会泄露某个用户是否参与了训练</li> </ol> <p><strong>样本级别</strong>(即CCS2016,deep learning with differential privacy)：</p> <blockquote> <p>是在通常的SGD的一个batch训练过程中，增加了DP的两个步骤。在一个batch根据损失函数计算完梯度之后，在进行梯度下降的更新前，第一步是对每个样本计算的梯度裁剪，第二步是在这个batch的梯度更新前对梯度更新总值添加噪声。对梯度进行放缩，让梯度的二范数值在范围C内，之后添加的噪声的大小和C值有关。C值是一个动态变化的值，文章中提出，C的值选取为这个batch梯度的范数中位数值</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230315101320207.png" alt="image-20230315101320207" style="zoom:25%;"/></p> <p>联邦学习场景下的<strong>用户级别</strong>差分隐私，包括三个步骤，①裁剪②聚合③加噪</p> <blockquote> <p>联邦聚合FedAvg，每个用户返回梯度的更新值\(\theta^\prime-\theta_t\)给服务器，服务器做加权聚合</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230315101714927.png" alt="image-20230315101714927" style="zoom:25%;"/></p> <p>联邦学习的差分隐私包含两个步骤都是由服务器完成：①梯度裁剪②添加噪声</p> <blockquote> <p>客户端做完本地的SGD得到梯度更新值，服务器不是直接对梯度更新值进行聚合，而是进行梯度裁剪，与之前样本集的差分隐私类似，二范数限制取所有用户更新值的中位数值。</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230315111028793.png" alt="image-20230315111028793" style="zoom:25%;"/></p> <blockquote> <p>服务器对进行范数限制的梯度更新值进行加权聚合</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230315111449054.png" alt="image-20230315111449054" style="zoom:25%;"/></p> <blockquote> <p>第二个差分隐私步骤是添加噪声，添加噪声的强度与用户梯度更新范数值中位数S有关。在聚合平均之后，服务器添加高斯噪声，然后做模型参数的全局更新。</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230315133707442.png" alt="image-20230315133707442" style="zoom:25%;"/></p> <p><strong>完整算法：</strong></p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230315134106714.png" alt="image-20230315134106714" style="zoom:25%;"/></p>]]></content><author><name></name></author><category term="Differential-Privacy"/><category term="Privacy"/><summary type="html"><![CDATA[差分隐私与联邦学习的结合]]></summary></entry><entry><title type="html">An Efficient Framework for Clustered Federated Learning 阅读</title><link href="https://maozhenzhang.github.io/blog/2023/An-Efficient-Framework-for-Clustered-Federated-Learning/" rel="alternate" type="text/html" title="An Efficient Framework for Clustered Federated Learning 阅读"/><published>2023-03-24T00:00:00+00:00</published><updated>2023-03-24T00:00:00+00:00</updated><id>https://maozhenzhang.github.io/blog/2023/An%20Efficient%20Framework%20for%20Clustered%20Federated%20Learning</id><content type="html" xml:base="https://maozhenzhang.github.io/blog/2023/An-Efficient-Framework-for-Clustered-Federated-Learning/"><![CDATA[<h2 id="self-comprehension">Self Comprehension</h2> <p>作者将聚类任务放在客户端上，而不是服务器上(一次聚类)</p> <blockquote> <p>有点降低了服务器的计算，缺点时客户端需要上传自己所属的集群</p> </blockquote> <h2 id="introduction">Introduction</h2> <p>提出迭代联邦聚类算法(IFCA)，交替估计用户的集群身份并通过梯度下降算法优化用户集群的模型参数。</p> <blockquote> <p>在具有平方损失的线性模型中分析算法的收敛速度，分析通用的强凸函数和平滑损失函数</p> </blockquote> <h2 id="related-work">Related work</h2> <p><strong>从非 i.i.d. 中学习单一全局模型：</strong>[49、34、21、37、24、30]。[38、36、8]</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MOCHA，考虑多任务学习设置，形成一个确定性优化问题，其中用户的相关矩阵是正则化项，
</code></pre></div></div> <p><strong>通过全局模型微调进行个性化：</strong></p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>数据作为元学习问题 [4, 15, 8]。在此设置中，目标是首先获得一个全局模型，然后每个设备使用其本地数据微调模型。该公式的基本假设是不同用户之间的数据分布相似，全局模型可以作为一个很好的初始化
</code></pre></div></div> <p>[36][10]考虑了FL的制定，两项工作均采用集中式的聚类算法，如K-means</p> <blockquote> <p>其中服务器必须识别所有用户的集群身份，导致中心计算成本过高，这些算法可能不适用于深度神经网络等大型模型或具有大量模型的应用程序。</p> </blockquote> <p><strong>IFCA</strong></p> <p>潜在变量问题，作者提出的公式可以被视为分布式设置中具有潜在变量的统计估计问题</p> <blockquote> <p>而潜在变量是统计学和非凸优化中的经典话题，如高斯混合模型GMM[44、20]，线性回归混合模型[7、43、48]和相位检索[9,29]</p> </blockquote> <blockquote> <p>解决这些问题的两种流行方法是：期望最大化(EM)和交替最小化（AM）</p> </blockquote> <p>近年来EM和AM在集中式环境中的融合方面的理解取得了一些进展[31, 5, 47, 1, 42]</p> <blockquote> <p>如从一个合适的点开始，它们的收敛速度会很快，有时它们会有超线性的收敛速度[44, 11]</p> </blockquote> <h2 id="problem-formulation">Problem formulation</h2> <p>集群划分：\(m\)台机器划分为\(k\)个不相交的集群，假设不知道每台机器所属的集群</p> <p>集群内数据点：假设每个客户端\(i\in S^*_j\)包含数据集\(\mathcal D_j\)的\(n\)个\(i.i.d\)数据点\(\{z^{i,1},z^{i,2},...,z^{i,n}\}\)，</p> <blockquote> <p>每个数据点\(z^{i,j}\)由特征和相应的标签组成\(z^{i,\ell}=(x^{i,\ell},y^{i,\ell})\)</p> </blockquote> <p>让\(f(\theta;z)：\Theta\rightarrow\mathbb R\)表示与数据点\(z\)相关的损失函数，\(\Theta\subseteq \mathbb R^d\)是参数空间【作者选择\(\Theta= \mathbb R^d\)】</p> <p>作者的目标是最小化对所有\(j\in[k]\)的总体损失\(F^j(\theta):=\mathbb E_{z\sim\mathcal D_j}[f(\theta;z)]\)</p> <blockquote> <p>拓展：这里的\(E_{z\sim\mathcal D_j}[f(\theta;z)]\)表示函数\(f\)关于样本空间\(\mathcal D_j\)的期望，指的是最大似然估计(Maximum Likelihood Estimation)，在数据集合\(\mathcal D_j\)已知的前提下， 对于函数\(f\)的参数\(\theta\)的极大似然估计值。</p> <p>换句话说，希望可以得到最小化期望时的\(\theta\)值</p> </blockquote> <p>试图找到\(\{\hat \theta\}^k_{j=1}\)使得\(\theta^*_j=\text {argmin}_{\theta\in\Theta}F^j(\theta),j\in[k]\)</p> <p>原文：”因为只有有限数据，所以利用了经验损失函数“</p> <blockquote> <p>不太理解这句话，如果不是因为这个还有其他的可选么</p> </blockquote> <p>\(Z\subseteq\{z^{i,1},z^{i,2},...,z^{i,n}\}\)是第\(i\)台机器上数据点的子集</p> <blockquote> <p>或者说：第\(i\)台机器上的数据点\(Z\subseteq\{z^{i,1},z^{i,2},...,z^{i,n}\}\)是子集?</p> </blockquote> <p>定义经验损失为\(F_i(\theta;Z)=\frac{1}{\vert Z\vert}\sum_{z\in Z}f(\theta;z)\)，用\(F_i(\theta)\)来表示第\(i\)-th worker的经验损失</p> <h2 id="algorithm">Algorithm</h2> <p>算法主要思想：估计集群身份时交替使用最小化损失函数，讨论了IFCA的两种变体，即梯度平均和模型平均</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230322165524883.png" alt="image-20230322165524883" style="zoom:50%;"/></p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230322165302302.png" alt="image-20230322165302302" style="zoom: 25%;"/></p> <p>算法从\(k\)个模型参数参数开始，\(\theta^{(0)}_j,j\in[k]\)。在第\(t\)-th迭代中，IFCA的中心服务器随机选择客户端的子集，\(M_t\in[m]\)并广播当前模型参数\(\{\theta_j^{(t)}\}_{j=1}^k\)给\(M_t\)中的用户（称\(M_t\)为参与了设备的集合）</p> <p><strong>通过参数评估</strong>：每个客户端都配备了局部经验损失函数\(F(\cdot)\)使用收到的参数评估\(F_i\)，第\(i\)-th个客户端(\(i\in M_t\))==通过寻找最低损失的模型参数来估计其所在的集群标识==，如\(\hat j=\text{argmin}_{j\in[k]}F_i(\theta^{(t)}_j)\)</p> <p><strong>通过梯度平均</strong>：如果选择梯度平均选项，客户端讲计算参数\(\theta^{(t)}_{\hat j}\)的局部经验损失\(F_i\)的随机梯度，并将其集群身份估计和梯度法送回中心机器。</p> <blockquote> <p>\(m\)是所有客户端、\(M_t\)是随机挑选的部分客户端(子集)、\(k\)是集群个数、\(\theta_j^{(0)}\)是某一个集群的全局模型参数</p> <p>【理解】：使用损失的多少来作为分类标准（如普通k-means的欧氏距离）</p> </blockquote> <p>中心服务器接收到所有客户端的梯度和集群身份估计，收集集群身份估计相同的客户端，对各相应集群中的模型参数进行梯度下降更新，如果选择平均选项(类似于联邦平均算法[27])，每个参与的设备需要运行\(\tau\)次本地随机梯度下降更新，并且发送新模型及其集群对服务器的身份估计。然后中心服务器对集群身份估计相同的客户端新模型进行平均</p> <h3 id="具体实施多任务学习权重共享">具体实施（多任务学习权重共享）</h3> <p>集群结构可能是模糊的，这意味着尽管来自不同集群的数据分布不同，但模型应该利用所有用户数据的一些共同属性。</p> <blockquote> <p>基于此，使用”多任务学习权重共享技术[3]”</p> </blockquote> <p>具体而言，训练神经网络模型时，在所有集群之间共享前几层的权重，这样可以通过可获得的数据学习到良好的表示。</p> <p>然后仅在最后（或最后几层上）运行IFCA算法以解决不同集群中相应的不同的分布。</p> <p>使用算法1中与\(\theta_j^{(t)}\)相符的子集运行IFCA，对剩余的运行联邦平均或vanilla梯度平均</p> <p>这样的好处是中心服务器不需要发送\(k\)个模型给所有机器，只需要发送所有权重的一个子集，它具有\(k\)个不同版本，以及一个共享的副本层</p> <p>当集群身份的估计在几次并行迭代中没有改变认为稳定</p> <h2 id="theoretical-guarantees">Theoretical guarantees</h2> <p>总共有\(T\)次并行迭代，将每个客户端上的\(n\)个数据点划分为\(2T\)次不相交的子集，每个子集有\(n^\prime=\frac{n}{2T}\)个数据</p> <blockquote> <p>如第\(i\)-个客户端上，使用子集\(\widehat Z^{(0)}_i,...,\widehat Z^{(T-1)}_i\)聚类，使用子集\(Z^{(0)}_i,...,Z^{(T-1)}_i,\)梯度下降</p> </blockquote> <p>使用\(\hat Z_i^{(t)}\)来聚类，用\(Z_i^{(t)}\)来计算梯度</p> <p>算法的每次迭代使用新的样本数据，使用心得数据点集获得聚类估计并计算梯度</p> <blockquote> <p>目的是消除聚类估计和梯度计算之间的相互依赖，并确保在每次迭代中使用新的独立同分布</p> </blockquote> <p>即第\(j\)个簇的参数向量的更新规则为：</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230325232437953.png" alt="image-20230325232437953" style="zoom:50%;"/></p> <p>\(S^{(t)}_j\)表示第\(t\)次迭代中集群身份估计为\(j\)的客户端集合，后续作者讨论了两种模型下的收敛性保证:</p> <ul> <li>5.1节分析了具有高斯特征和平方损失的线性模型下的算法</li> <li>5.2节分析了强凸损失函数的常规设置下的算法</li> </ul> <p>在第 5.2 节中讨论了广泛研究的混合线性回归问题 [46、47] 的分布式公式。</p> <hr/> <h3 id="线性模型的平方损失">线性模型的平方损失</h3> <p>假设第\(j\)-簇聚类中客户端的数据产生的方式是：\(i\in S^*_j\)，第\(i\)个客户端的特征反应满足 \(y^{i,\ell}=\langle x^{i,\ell},\theta^*_j\rangle+\epsilon^{i,\ell}\) 其中\(x^{i,\ell}~\sim\mathcal N(0,I_d)\)，独立于\(x^{i,\ell}的\)加性噪声(additive noise)\(\epsilon^{i,\ell}\sim\mathcal N(0,\sigma^2)\)，如我们所见，该模型是分布式线性回归模型的混合，在上述设置下， 参数\(\{\theta^*_j\}^k_{j=1}\)是总损失函数\(F^j(\cdot)\)的最小值</p> <blockquote> <p>加性噪声\(\epsilon^{i,\ell}\)与\(x^{i,\ell}的\)无关</p> </blockquote> <ol> <li> <p>将\(p_i:=\vert S^*_j\vert/m\)作为第\(j\)个聚簇中客户端数量占总数的比例，</p> </li> <li> <p>并让\(p:=\min\{p_1,p_2,...,p_k\}\)，</p> </li> <li> <p>同样定义最小分离\(\Delta\)，\(\Delta:=\min_{j\not=j^\prime\Vert \theta^*_j-\theta^*_{j^\prime}\Vert}\)，\(\rho:=\frac{\Delta^2}{\sigma^2}\)作为信噪比</p> </li> </ol> <p>在确定收敛结果前，陈述一些假设，回想\(n^\prime\)表示每步操作中每个客户端的数据数量</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326095722576.png" alt="image-20230326095722576" style="zoom:50%;"/></p> <p>在假设中，作者假设初始化足够接近\(\theta^*_j\)，并指出到这是混合模型[1, 45]的收敛分析中的标准假设，因为这是混合模型问题的非凸优化，在假设2中，我们对\(n^\prime,m,p,d\)做出了温和的假设。条件\(pmn^\prime\gtrsim d\)的简单假设，我们每次迭代时对于每个集群全部数据的大小至少和参数空间维度一样大，条件\(\)\Delta \gtrsim \frac{\sigma}{p} \sqrt{\frac{d}{m n^{\prime}}}+\exp \left(-c\left(\frac{\rho}{\rho+1}\right)^2 n^{\prime}\right)\(\)保证迭代接近\(\theta^*_j\)</p> <p>作者对算法进行了单步分析，假设在某个迭代中，获得了接近真实\(\theta^*_j\)的参数向量\(\theta_j\)，并且表明\(\theta_j\)以指数速率收敛于\(\theta^*_j\)并带有一个误差底线</p> <blockquote> <p>作者假设初始化足够接近\(\theta^*_j\)，这是混合模型[1,45]收敛分析的标准假设，</p> <p>作者在假设2中，对\(n^\prime,m,p,d\)做出了宽松的假设，\(pmn^\prime\gtrsim d\)，\(d\)是参数的维度</p> </blockquote> <p><strong>Theorem 1.</strong>考虑线性模型并假设Assumptions 1和2成立。假设在某次迭代中得到的参数向量\(\theta_j\)满足\(\Vert\theta_j-\theta^*_j\Vert\le\frac{1}{4}\Delta\)</p> <p>让\(\theta^+_j\)表示这次迭代后的向量，存在通用常数\(c_1,c_2,c_3,c_4&gt;0\)，这样当我们选择步长\(\gamma=c_1/p\)的概率至少为\(1-1/\text{poly}(m)\)，对所有的\(j\in[k]\)，我们有：</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326113410093.png" alt="image-20230326113410093" style="zoom:50%;"/></p> <p>附录中证明了定力1，简要总结思路：</p> <ul> <li>使用初始化条件，作者表明集合\(\{S_j\}^k_{j=1}\)与\(\{S^*_j\}^k_{j=1}\)有显著的重叠</li> <li>在重叠集合中，认为因为线性回归的基本属性，梯度的步骤存在了收缩和误差。</li> <li>然后作者限制了错误分类客户端的梯度范数并将它们添加到误差层</li> <li>作者通过结合正确分类和错误分类的客户端的贡献来证明</li> <li>迭代应用Therem1并在一下推论中获得最终解\(\widehat \theta_j\)的精度</li> </ul> <p><strong>Corollary 1.</strong> 考虑到线性模型并假设Assumptions 1和2成立。通过选择\(\gamma=c_1/p\)的概率至少为\(\)1-\frac{\log (\Delta / 4 \varepsilon)}{\operatorname{poly}(m)}\(\),在并行迭代\(T=\log\frac{\Delta}{4\epsilon}\)次后，我们对于所有\(j\in[k]\)，有\(\Vert\theta_j-\theta^*_j\Vert\le\frac{1}{4}\Delta\)，其中\(\)\varepsilon=c_5 \frac{\sigma}{p} \sqrt{\frac{d}{m n^{\prime}}}+c_6 \exp \left(-c_4\left(\frac{\rho}{\rho+1}\right)^2 n^{\prime}\right)\(\)</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326161424049.png" alt="image-20230326161424049" style="zoom:50%;"/></p> <p>检验最终的正确性：</p> <p>由于每个客户端的数据点\(n=2n^\prime T=2n^\prime\log(\Delta/4\epsilon)\)，我们知道对于最小的集群，总共有\(2pmn^\prime\log(\Delta/4\epsilon)\)个数据点，根据线性回归的minimax rate[41]，我们能知道即使知道真实的聚类身份，我们也无法获得比\(\mathcal O(\sigma\sqrt{\frac{d}{pmn^\prime\log(\Delta/4\epsilon)}})\)更优的错误率。我们统计正确率\(\epsilon\)与此错误率相比，可以看到第一项\(\frac{\sigma}{p}\sqrt\frac{d}{mn^\prime}\)在\(\epsilon\)中与minimax rate相当，只是存在一个对数因子和一个关于数据维度\(p\)的依赖关系。同时，该算法的第二项误差随着样本量\(n^\prime\)的增加而指数级衰减，因此最终的统计误差接近最优的水平</p> <hr/> <h3 id="strongly-convex-loss-functions">Strongly convex loss functions</h3> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326150844392.png" alt="image-20230326150844392" style="zoom:50%;"/></p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326150908592.png" alt="image-20230326150908592" style="zoom:50%;"/></p> <p>Assumption 3:假设总的损失函数\(F^j(\theta)\)是强凸函数和光滑的，不会对单个损失函数\(f(\theta,z)\)做出convexity或smoothness的假设，相反对\(f(\theta;z)\)和\(\nabla f(\theta;z)\)下的分布假设：</p> <p>Assumption 4:对于每个\(\theta\)和\(j\in[k]\)，当\(z\)是\(\mathcal D_j\)的随机采样，\(\eta^2\)是\(f(\theta;z)\)的方差上界，即\(\)\mathbb{E}_{z \sim \mathcal{D}_j}\left[\left(f(\theta ; z)-F^j(\theta)\right)^2\right] \leq \eta^2\(\)</p> <p>Assumption 5:对于每个\(\theta\)和\(j\in[k]\)，当\(z\)是\(\mathcal D_j\)的随机采样，\(v^2\)是\(\nabla f(\theta;z)\)的方差上界，即\(\)\mathbb{E}_{z \sim \mathcal{D}_j}\left[\Vert\nabla f(\theta ; z)-\nabla F^j(\theta)\Vert^2_2\right] \leq v^2\(\)</p> <p>梯度的有界方差在分析SGD[6]中非常常见。</p> <p>本文中作者使用损失函数来确定聚类身份，因此还需要对\(f(\theta;z)\)的进行概率假设。作者表明方差的有界性约束是相对较弱的假设约束，除了上述的假设，仍然使用5.1节中的一些定义，如：</p> <ul> <li> <p>最小间隔\(\Delta\)，\(\Delta:=\min_{j\not=j^\prime\Vert \theta^*_j-\theta^*_{j^\prime}\Vert}\)</p> </li> <li> <p>将\(p_i:=\vert S^*_j\vert/m\)，</p> </li> </ul> <p>​ 作为第\(j\)个聚簇中客户端数量占总数的比例，</p> <ul> <li> \[p:=\min\{p_1,p_2,...,p_k\}\] </li> </ul> <p>对初始化\(n^\prime,p,\Delta\)做出如下假设：</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326152538630.png" alt="image-20230326152538630" style="zoom:50%;"/></p> <p>为了简单起见，\(\widetilde {\mathcal O}\) 符号省略了任何不依赖于\(m\)和\(n^\prime\)的对数因子和数量，正如我们所见的，我们需要假设良好的初始化，因为混合模型的性质和我们对\(n^\prime,p,\Delta\)相对宽松的假设，特别是\(\Delta\)假设确保了迭代失踪保持靠近在\(\theta^*_j\)的\(\ell_2\)距离的球面上。</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326154556848.png" alt="image-20230326154556848" style="zoom:50%;"/></p> <p>假设Assumptions 3-6成立，选择步长\(\gamma=1/L\)，然后在概率至少为\(1-\delta\)的情况下，并行迭代\(T=\frac{8L}{p\lambda\log(\frac{\Delta}{2\epsilon})}\)次，对所有\(j\in[k],\Vert\widehat \theta_j-\theta^*_j\Vert\le\epsilon\)，其中</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326155009530.png" alt="image-20230326155009530" style="zoom:50%;"/></p> <p>附录B中证明了定理2，与5.1节类似，为了证明这个结果，首先需要证明每次迭代的收缩：</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230326155132692.png" alt="image-20230326155132692" style="zoom:50%;"/></p> <p>然后获得收敛速度，为了更好的解释结果，作者关注于对m和n的依赖性，并将其他量视为常熟，然后由于\(n=2n^\prime T\)，我们知道\(n\)和\(n^\prime\)在对数银子上具有相同的比例。因此可以得到最终的计算误差为\(\)\epsilon=\widetilde{\mathcal{O}}\left(\frac{1}{\sqrt{m n}}+\frac{1}{n}\right)\(\)。如5.1节所述，即使知道集群身份，\(\frac{1}{\sqrt{mn}}\)也是最佳速率，因此作者的统计率在接近\(n\gtrsim m\)的情况下接近最优。于线性模型中的统计率相比\(\)\widetilde{\mathcal{O}}\left(\frac{1}{\sqrt{m n}}+\exp (-n)\right)\(\)，作者注意到主要区别在于第二项。线性模型和强凸情况下的附加项分别是\(\exp(-n)\)和\(\frac{1}{n}\)，作者注意到这是因为由于不同的统计假设：线性模型中，假设高斯噪声，而强凸情况下，只假设有界方差。</p> <hr/> <h2 id="实验">实验</h2> <p>不会再每次迭代时重新采样新数据点，此外，还可以放宽初始化要求</p> <blockquote> <p>More specifically，对于线性模型，我们观察到随机初始化和几次重新启动足以确保算法1的收敛</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230323102018587.png" alt="image-20230323102018587" style="zoom:33%;"/></p> <blockquote> <p>不同成功的概率:</p> <ul> <li>(a)、(b)是分离尺度\(R\)和加性噪声\(\sigma\)</li> <li>(c)、(d)是客户端数量\(m\)和每个客户端采样大小\(n\)</li> </ul> <p>(a)和(b)中，随着\(R\)的增加成功的概率增加，即，基本反映出真实参数向量的之间的距离程度</p> <p>(c)和(d)中，随着\(mn\)的增加成功的概率在提升，即每个客户端的数据更多/客户端的数量更多，成功的概率也会提升</p> </blockquote> <h3 id="生成数据">生成数据</h3> <p>首先在具有平方损失的线性模型上使用梯度平均(选项I)评估算法,</p> <p>首先生成\(\theta^*_j\sim\text{Bernoulli(0.5)}的值，并且将它们的\)\(\ell_2\)范数作为\(R\)，这确保了\(\theta^*_j\)之间的间距与\(R\)成正比期望</p> <blockquote> \[\theta^*_1,...,\theta^*_j\] </blockquote> <p>每次实验中，首先生成参数向量\(\theta^*_j\)并固定他们，根据独立的伯努利分布对应的随机初始化\(\theta^{(0)}_j\)</p> <p>运行算法1，300次迭代，步长不变。对于\(k=2\)和\(k=4\)，分别在\(\{0.01,0.1,1\}\)和\(\{0.5,1.0,2\}\)中选择步长。为了确定是否成功学习了模型，我们回到上述步长并定义距离的度量：\(\text{dist}=\frac{1}{k}\sum^k_{j=1}\Vert \hat \theta_j-\theta^*_j\Vert\)，其中\(\{\hat \theta_j\}^k_{j=1}\)是从算法1中获得的参数估计，如果对于\(\theta^*_j\)的固定集合，在10个随机初始化参数\(\theta_j^{(0)}\)中，至少在一个场景中获得\(\text {dist}\le0.6\sigma\)，则实验称为成功</p> <p>在图2(a-b)中，针对分离参数\(R\)绘制了40次实验的经验成功概率。将问题参数设置为</p> <ol> <li>(a), \(k=2并且(m,n,d)=(100,100,1000)\)</li> <li>(b), \(k=4并且(m,n,d)=(400,100,1000)\)</li> </ol> <p>正如所看到的，当\(R\)变大时，即参数之间的距离会变大，问题变得更容易解决，成功概率更高。这验证了作者的理论结果，更高的信噪比产生更小的误差层。</p> <p>在图2(c-d)中，作者描述了对\(m,n\)的依赖性，将\(R\)和\(d\)固定为</p> <ol> <li>(c), \((R,d)=(0.1,1000)\)</li> <li>(d), \((R,d)=(0.5,1000)\)</li> </ol> <p>观察到，当增加\(m\)或\(n\)的数量时，成功的概率会提高</p> <h3 id="mnist和cifar的旋转">MNIST和CIFAR的旋转</h3> <p>基于MNIST[19]和CIFAR-10[18]数据集创建了聚类FL数据集。为模拟不同机器上的数据从不同分布生成的环境，使用==旋转扩充数据集==</p> <p>并创建旋转MNIST[25]和旋转CIFAR数据集</p> <blockquote> <p>对MNIST应用0、90、180、270度旋转来扩充数据集，从而产生\(k=4\)个簇，对于给定的\(m和n\)满足\(mn=60000k\)，将图像随机划分为\(m\)个客户端，每个客户端上有\(n\)个具有相同旋转的图像。同样用相同的方式拆分测试数据集\(m_{test}=10000k/n\)个客户端</p> <p>对于CIFAR数据集，与MNIST相似操作，拆分主要区别在于创建了\(k=2\)个具有0度和190度旋转的簇，</p> </blockquote> <p>作者注意到，通过操纵MNIST和CIFAR-10等标准数据集来创建不同的任务已在持续学习研究社区中得到广泛采用[12、16、25]。对于集群FL，使用旋转创建数据集有助于我们模拟具有清晰集群结构的联邦学习设置。</p> <p>对于MNIST实验，使用具有ReLU激活的券链接神经网络，单个隐藏层大小为200，CIFAR实验，使用2个卷积层和2个全连接层组成的卷机神经网络模型，图像通过标准数据增强（翻转、随机裁剪）进行预处理</p> <p>作者将IFCA算法与两种基线算法进行比较，即全局模型和局部模型方案。对于IFCA，我们使用模型平均(算法1中的选项II)/</p> <blockquote> <p>对于MNIST实验，我们使用完整的客户端参与(对所有\(t\)，\(M_t=[m]\))。对于算法1的本地更新，我们选择\(\tau=10\)并且补偿\(\gamma=0.1\)。对于CIFAR实验，选择\(\vert M_t\vert=0.1m\)，并且应用下降的补偿0.99，还为LocalUpdate的过程设置\(\tau=5\)，batch size 50</p> <p>遵循之前的工作[28]（fedAvg）</p> <ol> <li>在全局模型方案中，该算法尝试学习一个单一的全局模型，该模型可以从所有分布中进行预测。该算法不考虑聚类身份，因此算法1中的模型平均的操作变成\(\theta^{(t+1)}=\sum_{i\in M_t}\hat \theta/\vert M_t\vert\)，即对所有参与机器参数进行平均。</li> <li>在局部模型的方案中每个解ID那种的模型仅对局部可用数据进行梯度下降，不进行模型平均</li> </ol> </blockquote> <p>对于IFCA和全局方案，通过以下方式进行推理：</p> <blockquote> <p>每台测试客户端，我们对所有学习模型(IFCA的\(k\)个模型和一个全局模型模型)进行推理，并从产生最小损失的模型计算准确率。</p> <p>为了测试局部模型基线，在相同分布的测试数据测试准确率(如那些旋转的数据)</p> </blockquote> <p>作者展示了客户端中所有模型的平均准确度，对于所有算法，使用5个不同的随机种子进行实验并报告平均值和标准偏差</p> <p>实验结果如表1所示，可以观察到作者的算法比两个基线性能更好，当运行IFCA算法时，作者观察到可以逐渐找到工作机器底层集群的标识，并且在找到正确的集群后，使用具有相同分布的数据训练和测试每个模型，从而获得更好的准确性。全局基线模型的性能比作者提出算法的性能更差。</p> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230324104723562.png" alt="image-20230324104723562" style="zoom:50%;"/></p> <h3 id="fedrated-emnist">Fedrated Emnist</h3> <p>作者在Federated Emnist(FEMNIST)[2]上提供了额外的实验结果，使用4.1节中提到的权重共享技术。</p> <p>使用具有两个卷机层+一个最大池化层+两个全连接层的神经网络，共享所有层的权重，除了IFCA训练的最后一层。将簇的数量\(k\)视为超参数，并使用不同的\(k\)进行实验，对比了具有IFCA的全局模型和局部模型的方法，并且还与一次性集中聚类算法进行了比较。测试精度如表2其中计算了5次独立运行的平均值和标准偏差。如所看到的，IFCA比全局模型和局部模型方法显出明显的优势，</p> <blockquote> <p>IFCA和One-shot算法的结果是相似的，但是如第2节中强调的，IFCA不运行集中式的聚类程序，因此降低了服务器的计算成本。</p> <p>最后，作者观察到IFC对于簇数\(k\)的选择是稳健的。\(k=2\)和\(k=3\)的算法结果类似，并且注意到当\(k&gt;3\)时，IFCA自动识别出3个簇，其余簇为空这表明IFCA在聚类结构不明确切簇类数量未知的现实问题中的适用性</p> </blockquote> <p><img src="https://mz-pico-1311932519.cos.ap-nanjing.myqcloud.com/image/image-20230324104741100.png" alt="image-20230324104741100" style="zoom:50%;"/></p> <h2 id="broader-impact">Broader Impact</h2> <p>作者提出它们的框架将更好地保护联邦学习系统中用户的隐私，同时仍提供个性化预测，</p> <blockquote> <p>不需要用户将自己的任何个人数据发送到中央服务器，用户仍然可以使用服务器的计算能力学习个性化模型</p> <p>一个潜在的风险是作者的算法仍然需要用户将集群身份发送到中央服务器。</p> </blockquote>]]></content><author><name>Avishek Ghosh</name></author><category term="Communication-Read"/><category term="Papers"/><summary type="html"><![CDATA[个性化模型学习,通过经验损失进行聚类]]></summary></entry></feed>